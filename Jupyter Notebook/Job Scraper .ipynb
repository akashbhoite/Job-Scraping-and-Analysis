{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Job Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a web scraper specially designed for scraping jobs from Linkedin.com\n",
    "- It can be used to scrape all the jobs from LinkedIn with a few tweaks \n",
    "- I have used it to scrape jobs related to my field of interest which is Data Science.\n",
    "  However, you can use it to scrape jobs related to your field of interest\n",
    "- Before you start scraping you will need to install Chromedriver on your system\n",
    "- After installing Chromedriver, please install all the necessary libraries using the \n",
    "  command **'pip install'**\n",
    "- Please note that, you will have to sign out of your LinkedIn account before scraping \n",
    "- After signing out, search for any job titles that you want to scrape and apply the  \n",
    "  desired filters (for ex: location, company, date posted, etc.)\n",
    "- When the search results appear, copy the URL and paste it in the **driver.get (\"URL\")*** \n",
    "  section of the code below and run the code\n",
    "- It might take a while, depending on the number of jobs in the search results. \n",
    "- You can find all the jobs with their **Title, Company Name, Location and Job\n",
    "  Description** in a csv file \n",
    "\n",
    "*Please note that, if you get an error (unable to find element) while running this code, you might need some html knowledge to fix it, because at times the html path is changed depending on the website's maintainence and updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(\"/Users/akashbhoite/Downloads/chromedriver\")\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/akashbhoite/Downloads/chromedriver\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.linkedin.com/jobs/search?keywords=Data%20Science&location=United%20States&trk=public_jobs_jobs-search-bar_search-submit&redirect=false&position=1&pageNum=0\")\n",
    "time.sleep(3)\n",
    "close = driver.find_element_by_xpath(\"/html/body/div[3]/button\")\n",
    "close.click()\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "# Get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "           \n",
    "        break\n",
    "    last_height = new_height\n",
    "    \n",
    "for i in range(100):\n",
    "    see_more = driver.find_element_by_xpath(\"//*[@id='main-content']/div/section/button\").click()\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #print(\"for ::   \",i)\n",
    "    time.sleep(2)\n",
    "\n",
    "    \n",
    "\n",
    "all_jobs = driver.find_elements_by_xpath(\"//*[@id='main-content']/div/section/ul/li\")\n",
    "#links = driver.find_elements_by_xpath(\"//*[@id='main-content']/div/section/ul/li/a\")\n",
    "#desc=[]\n",
    "#title_list = []\n",
    "\n",
    "\n",
    "    \n",
    "for job in all_jobs: \n",
    "    title1 = job.find_element_by_tag_name(\"h3\").text\n",
    "    company = job.find_element_by_tag_name(\"h4\").text\n",
    "    location = job.find_element_by_class_name(\"job-result-card__location\").text\n",
    "    try:\n",
    "        link = job.find_element_by_class_name(\"result-card__full-card-link\").click()\n",
    "    except:\n",
    "        continue\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        show_more = driver.find_element_by_xpath(\"//button[@aria-label='Show more']\").click()\n",
    "    except:\n",
    "        continue\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        desc2 = driver.find_element_by_xpath(\"//div[@class='show-more-less-html__markup']\").text.replace(\"\\n\",\"\").strip()\n",
    "    except:\n",
    "        desc2 = driver.find_element_by_xpath(\"//*[@id='main-content']/section/div[2]/section[2]/div/section/div\").text.replace(\"\\n\",\"\").strip()\n",
    "    df = df.append({\"Title\":title1, \"Company\":company, \"Location\":location,\"Description\":desc2},ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"linkedin.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
